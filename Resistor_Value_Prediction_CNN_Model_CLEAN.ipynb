{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqe_PrBTSBGM"
   },
   "source": [
    "# Resistor Value Prediction CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dz2KO84pSZgL"
   },
   "source": [
    "## 0. Getting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97XouzcVshYj",
    "outputId": "6515dbdc-b5f3-493e-bcf6-8d530f5ebe68"
   },
   "outputs": [],
   "source": [
    "# Mount our google drive to authorize access\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ob1t659AS1Wp",
    "outputId": "d1c6f15d-d9c7-4b62-f04b-a81e22424692"
   },
   "outputs": [],
   "source": [
    "# Import the need packages\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it does not work\n",
    "try:\n",
    "  from torchinfo import summary\n",
    "except:\n",
    "  print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "  !pip install -q torchinfo\n",
    "  from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory from Mr. Bourke's Pytorch training\n",
    "try:\n",
    "  from going_modular.going_modular import data_setup, engine\n",
    "  from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "  #Get the going modular_scripts\n",
    "  print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "  !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "  !mv pytorch-deep-learning/going_modular .\n",
    "  !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
    "  !rm -rf pytorch-deep-learning\n",
    "  from going_modular.going_modular import data_setup, engine\n",
    "  from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "iBKsfX6vUKNC",
    "outputId": "cb9a2f7c-cfa5-494f-bea7-bb1ca0c62d40"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO981qjIUdCu"
   },
   "source": [
    "## 1. Get and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GYQ6FpZhUsFG",
    "outputId": "e9a4d781-7b8f-4242-9eb6-b2ce5ff0f056"
   },
   "outputs": [],
   "source": [
    "#Install kaggle package\n",
    "!pip install kaggle\n",
    "\n",
    "# Upload my kaggle.json file\n",
    "from google.colab import files\n",
    "uploaded = files.upload() # Click \"Choose Files\" and select kaggle.json\n",
    "\n",
    "# Set up the credentials\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Now download your specific dataset\n",
    "!kaggle datasets download -d eralpozcan/resistor-dataset\n",
    "!unzip resistor-dataset.zip -d /content/resistor_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1YAI2q4YWIu",
    "outputId": "5cc7fbd5-a347-4a24-ed12-7f34195bad80"
   },
   "outputs": [],
   "source": [
    "# Combine all image files from my directory\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# Create main dataset directory\n",
    "main_dir = '/content/resistor_dataset/'\n",
    "os.makedirs(main_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def move_files(src_dir, dst_dir):\n",
    "  \"\"\"Find all image files recursively and move them\n",
    "  \"\"\"\n",
    "  for root, dirs, files in os.walk(src_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            src = os.path.join(root, file)\n",
    "            dst = os.path.join(dst_dir, file)\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "# Find all image files recursively and move them\n",
    "#move_files('resistor_dataset', main_dir)\n",
    "print(f\"Moved all images to {main_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNtJdbVrXoNF"
   },
   "outputs": [],
   "source": [
    "# Remove unecessary files\n",
    "\n",
    "!rm -f resistor-dataset.zip\n",
    "!rm -f *.jpg *.png *.csv *.txt\n",
    "!rm -rf kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "oCFlAUa-gSJE",
    "outputId": "61a38d61-d16b-4afa-dd5e-690f30e517c4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "\n",
    "\"\"\"\n",
    "# Split the data into train and test sets\n",
    "train_dir = '/content/resistor_dataset/train'\n",
    "test_dir = '/content/resistor_dataset/test'\n",
    "\n",
    "# Create the directory structure\n",
    "try:\n",
    "  os.makedirs(train_dir)\n",
    "  os.makedirs(test_dir)\n",
    "except FileExistsError:\n",
    "  print(\"Directory already exists\")\n",
    "\n",
    "\n",
    "# Get all image files from all_resistors\n",
    "all_images = [f for f in os.listdir\n",
    "\n",
    "\n",
    "\n",
    "    ('/content/resistor_dataset') if f.endswith(('.jpg','.png', '.jpeg'))]\n",
    "\n",
    "# Split the image names\n",
    "try:\n",
    "  train_images, test_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
    "  # Move training images\n",
    "  for img in train_images:\n",
    "   src = os.path.join('/content/resistor_dataset', img)\n",
    "   dst = os.path.join(train_dir, img)\n",
    "   shutil.move(src, dst)\n",
    "\n",
    "  # Move testing images\n",
    "  for img in test_images:\n",
    "    src = os.path.join('/content/resistor_dataset/*', img)\n",
    "    dst = os.path.join(test_dir, img)\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "  # Delete the old images\n",
    "  # Remove original images after copying\n",
    "  for file in glob.glob('/content/resistor_dataset/*/*.jpg'):\n",
    "    os.remove(file)\n",
    "  for file in glob.glob('/content/resistor_dataset/*/*.png'):\n",
    "    os.remove(file)\n",
    "except:\n",
    "    print(\"The data has already been split \")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ul6yltSPt63o",
    "outputId": "b566eac7-8971-405f-b3d1-f59f2b11ca4a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def simple_train_test_split(dataset_path, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Simple function to split dataset into train/test\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "\n",
    "    # Create train and test directories\n",
    "    train_dir = dataset_path.parent / \"all_resistors\" / \"train\"\n",
    "    test_dir = dataset_path.parent / \"all_resistors\" / \"test\"\n",
    "\n",
    "    train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process each class folder\n",
    "    for class_folder in dataset_path.iterdir():\n",
    "        if not class_folder.is_dir():\n",
    "            continue\n",
    "\n",
    "        class_name = class_folder.name\n",
    "        print(f\"Processing class: {class_name}\")\n",
    "\n",
    "        # Create class subdirectories\n",
    "        (train_dir / class_name).mkdir(exist_ok=True)\n",
    "        (test_dir / class_name).mkdir(exist_ok=True)\n",
    "\n",
    "        # Get all images in this class\n",
    "        images = list(class_folder.glob(\"*.jpg\")) + list(class_folder.glob(\"*.png\"))\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Split images\n",
    "        split_point = int(len(images) * train_ratio)\n",
    "        train_images = images[:split_point]\n",
    "        test_images = images[split_point:]\n",
    "\n",
    "        # Copy to train folder\n",
    "        for img in train_images:\n",
    "            shutil.copy2(img, train_dir / class_name / img.name)\n",
    "\n",
    "        # Copy to test folder\n",
    "        for img in test_images:\n",
    "            shutil.copy2(img, test_dir / class_name / img.name)\n",
    "\n",
    "        print(f\"  Train: {len(train_images)}, Test: {len(test_images)}\")\n",
    "\n",
    "# Use it like this:\n",
    "simple_train_test_split(\"/content/resistor_dataset\", train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7YxCWQeinQg"
   },
   "source": [
    "### 1.2 Create transforms for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nl9RJaT-lKPN"
   },
   "outputs": [],
   "source": [
    "# We are going to be using four models and select the best performing model for deployment\n",
    "  # - ResNet-18\n",
    "  # - ResNet34\n",
    "  # - EffNEtB0\n",
    "  # - EffNetB2\n",
    "  # - MobileNetV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Setup pretrained weights\n",
    "resnet18_weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
    "resnet34_weights = torchvision.models.ResNet34_Weights.DEFAULT\n",
    "effnetb0_weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "mobilenetv2_weights = torchvision.models.MobileNet_V2_Weights.DEFAULT\n",
    "\n",
    "# Get the transforms for each model from the weights\n",
    "resnet18_transforms = resnet18_weights.transforms()\n",
    "resnet34_transforms = resnet34_weights.transforms()\n",
    "effnetb0_transforms = effnetb0_weights.transforms()\n",
    "effnetb2_transforms = effnetb2_weights.transforms()\n",
    "mobilenetv2_transforms = mobilenetv2_weights.transforms()\n",
    "\n",
    "\n",
    "def train_transform(base_transform: transforms.Compose):\n",
    "  \"\"\"Create a training transform with data augmentation\n",
    "  \"\"\"\n",
    "  transform = transforms.Compose([\n",
    "      transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "      transforms.RandomHorizontalFlip(p=0.5),\n",
    "      transforms.RandomVerticalFlip(p=0.5),\n",
    "      transforms.RandomRotation(degrees=(-30, 30)),\n",
    "      transforms.ToTensor(),\n",
    "      base_transform\n",
    "  ])\n",
    "\n",
    "  return transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNkhG4a8jt-g",
    "outputId": "9929b5a1-17dd-4500-9bbe-a54ecdb61e18"
   },
   "outputs": [],
   "source": [
    "resnet18_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EN1jSgWoDhG"
   },
   "source": [
    "### 1.3 Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXidkmXnrjNb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders for\n",
    "image classification data.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str,\n",
    "    test_dir: str,\n",
    "    train_transform: transforms.Compose,\n",
    "    test_transform: transforms.Compose,\n",
    "    batch_size: int,\n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "  \"\"\"Creates training and testing DataLoaders.\n",
    "\n",
    "  Takes in a training directory and testing directory path and turns\n",
    "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "\n",
    "  Args:\n",
    "    train_dir: Path to training directory.\n",
    "    test_dir: Path to testing directory.\n",
    "    transform: torchvision transforms to perform on training and testing data.\n",
    "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "    num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "    Where class_names is a list of the target classes.\n",
    "    Example usage:\n",
    "      train_dataloader, test_dataloader, class_names = \\\n",
    "        = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                             test_dir=path/to/test_dir,\n",
    "                             transform=some_transform,\n",
    "                             batch_size=32,\n",
    "                             num_workers=4)\n",
    "  \"\"\"\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJ-yRriIoH9r"
   },
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "BATCH_SIZE=32\n",
    "train_dir = '/content/all_resistors/train'\n",
    "test_dir = '/content/all_resistors/test'\n",
    "\n",
    "\n",
    "# Create individual dataloaders for each model\n",
    "resnet18_train_dataloader, resnet18_test_dataloader, resnet18_class_names = create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                          test_dir=test_dir,\n",
    "                                                                                                          train_transform=train_transform(resnet18_transforms),\n",
    "                                                                                                          test_transform=resnet18_transforms,\n",
    "                                                                                                          batch_size=BATCH_SIZE)\n",
    "\n",
    "resnet34_train_dataloader, resnet34_test_dataloader, resnet34_class_names = create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                          test_dir=test_dir,\n",
    "                                                                                                          train_transform=train_transform(resnet34_transforms),\n",
    "                                                                                                          test_transform=resnet34_transforms,\n",
    "                                                                                                          batch_size = BATCH_SIZE)\n",
    "\n",
    "effnetb0_train_dataloader, effnetb0_test_dataloader, effnetb0_class_names = create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                          test_dir=test_dir,\n",
    "                                                                                                          train_transform=train_transform(effnetb0_transforms),\n",
    "                                                                                                          test_transform=effnetb0_transforms,\n",
    "                                                                                                          batch_size = BATCH_SIZE)\n",
    "\n",
    "effnetb2_train_dataloader, effnetb2_test_dataloader, effnetb2_class_names = create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                          test_dir=test_dir,\n",
    "                                                                                                          train_transform=train_transform(effnetb2_transforms),\n",
    "                                                                                                          test_transform=effnetb2_transforms,\n",
    "                                                                                                          batch_size=BATCH_SIZE)\n",
    "mobile_train_dataloader, mobile_test_dataloader, mobile_class_names = create_dataloaders(train_dir= train_dir,\n",
    "                                                                                                    test_dir=test_dir,\n",
    "                                                                                                    train_transform=train_transform(mobilenetv2_transforms),\n",
    "                                                                                                    test_transform=mobilenetv2_transforms,\n",
    "                                                                                                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nk3iH0pkr_aA",
    "outputId": "c17e07ad-0645-4250-8e05-9db51ef5036f"
   },
   "outputs": [],
   "source": [
    "len(mobile_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFwhpZeygmBk",
    "outputId": "c1aa40ed-2583-4a5a-975e-6ad80056456b"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.mobilenet_v2(weights=mobilenetv2_weights).to(device)\n",
    "\n",
    "summary(model, (32, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDppQghBftPC"
   },
   "source": [
    "### 1.4 Creating the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8lkPC7OfxcP"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "OUT_FEATURES = len(resnet18_class_names)\n",
    "\n",
    "def create_resnet18():\n",
    "  # 1. Get the base model with pretrained weights and send it to target device\n",
    "  weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
    "  model = torchvision.models.resnet18(weights=weights).to(device)\n",
    "\n",
    "  # 2. Freeze the base model layers\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  # 3. Set the seeds\n",
    "  set_seeds()\n",
    "\n",
    "  # 4. Change the classifier head\n",
    "  model.fc = nn.Linear(in_features=512, out_features=37, bias=True)\n",
    "\n",
    "\n",
    "  # 5. give the model a name\n",
    "  model.name = \"resnet18\"\n",
    "  print(f\"[INFO] Created new {model.name} model.\")\n",
    "  return model\n",
    "def create_resnet34():\n",
    "    weights = torchvision.models.ResNet34_Weights.DEFAULT\n",
    "    model = torchvision.models.resnet34(weights=weights).to(device)\n",
    "\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    set_seeds()\n",
    "\n",
    "    model.fc = nn.Linear(in_features=512, out_features=OUT_FEATURES, bias=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create an EffNetB0 feature extractor\n",
    "def create_effnetb0():\n",
    "  # 1. Get the base model with pretrained weights and sent it to target device\n",
    "  weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "  model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "  # 2. Freeze the base model Layers\n",
    "  for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  # 3. Set the seeds\n",
    "  set_seeds()\n",
    "\n",
    "  # 4. Change the classfier head\n",
    "  model.classifier = nn.Sequential(\n",
    "      nn.Dropout(p=0.3, inplace=True),\n",
    "      nn.Linear(in_features=1280, out_features=OUT_FEATURES, bias=True))\n",
    "\n",
    "  # 5. Give the model a name\n",
    "  model.name = \"effnetb0\"\n",
    "  print(f\"[INFO] Created new {model.name} model.\")\n",
    "  return model\n",
    "\n",
    "# Create an EffNetB2 feature extractor\n",
    "def create_effnetb2():\n",
    "  # 1. Get the base model with pretrained weights and send to target device ]\n",
    "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "  model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "\n",
    "  # 2. Freeze the base model layers\n",
    "  for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  # 3. Set the seeds\n",
    "  set_seeds()\n",
    "\n",
    "  # 4. Change the classifier head\n",
    "  model.classifier = nn.Sequential(\n",
    "      nn.Dropout(p=0.3, inplace=True),\n",
    "      nn.Linear(in_features=1408, out_features=OUT_FEATURES, bias=True))\n",
    "\n",
    "  # 5. Give the model a name\n",
    "  model.name = \"effnetb2\"\n",
    "  print(f\"[INFO] Created new {model.name} model.\")\n",
    "  return model\n",
    "\n",
    "# Create a MobileNetV2 feature extractor\n",
    "def create_mobilenetv2():\n",
    "  # 1. Get the base model with pretrained weights and send to target device\n",
    "  weights = torchvision.models.MobileNet_V2_Weights.DEFAULT\n",
    "  model = torchvision.models.mobilenet_v2(weights=weights).to(device)\n",
    "\n",
    "  # 2. Freeze the base model layers\n",
    "  for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  # 3. Set the seeds\n",
    "  set_seeds()\n",
    "\n",
    "  #4. Change the classifier head\n",
    "  model.classifier = nn.Sequential(\n",
    "      nn.Dropout(p=0.2, inplace=True),\n",
    "      nn.Linear(in_features=1280, out_features=OUT_FEATURES, bias=True))\n",
    "  # 5. Give the model a name\n",
    "  model.name = \"mobilenetv2\"\n",
    "  print(f\"[INFO] Created new {model.name} model.\")\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b0EPJQSjfWe",
    "outputId": "1750c24e-d131-4483-92dc-e96fa282fb84"
   },
   "outputs": [],
   "source": [
    "resnet18 = create_resnet18()\n",
    "resnet34 = create_resnet34()\n",
    "effnetb0 = create_effnetb0()\n",
    "effnetb2 = create_effnetb2()\n",
    "mobilenetv2 = create_mobilenetv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMsL17DG8gr2",
    "outputId": "4d6af87d-885b-42ee-cf2a-bb720ac62628"
   },
   "outputs": [],
   "source": [
    "summary(mobilenetv2, (32, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0vYlCv1lDUq"
   },
   "source": [
    "### 1.5 Create Writer\n",
    "\n",
    "This will allow us to track the results from our experiment on the tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1CFm39WV2-p"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rp0FR972Srqx"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "def create_writer(experiment_name: str,\n",
    "                  model_name: str,\n",
    "                  extra: str=None) -> SummaryWriter():\n",
    "\n",
    "  \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
    "\n",
    "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
    "\n",
    "    Where timestamp is the current date in YYYY-MM-DD format.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name of experiment.\n",
    "        model_name (str): Name of model.\n",
    "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
    "\n",
    "    Example usage:\n",
    "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
    "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
    "                               model_name=\"effnetb2\",\n",
    "                               extra=\"5_epochs\")\n",
    "        # The above is the same as:\n",
    "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
    "    \"\"\"\n",
    "\n",
    "  from datetime import datetime\n",
    "  import os\n",
    "\n",
    "  # Get timestamp of current date (all experiments on certain day like in same folder)\n",
    "  timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "  if extra:\n",
    "    log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "  else:\n",
    "    log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "\n",
    "  print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
    "  return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGNnCoVZVIx9"
   },
   "source": [
    "### 1.6 Create training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rIDxtXkSzTW"
   },
   "outputs": [],
   "source": [
    "from going_modular.going_modular.engine import train_step, test_step\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          writer: SummaryWriter,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for\n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "              train_acc: [...],\n",
    "              test_loss: [...],\n",
    "              test_acc: [...]}\n",
    "    For example if training for epochs=2:\n",
    "             {train_loss: [2.0616, 1.0537],\n",
    "              train_acc: [0.3945, 0.3945],\n",
    "              test_loss: [1.2641, 1.5706],\n",
    "              test_acc: [0.3400, 0.2973]}\n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Make sure model on target device\n",
    "    model.to(device)\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "      # Add loss results to SummaryWriter\n",
    "        writer.add_scalars(main_tag=\"Loss\",\n",
    "                           tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                            \"test_loss\": test_loss},\n",
    "                           global_step=epoch)\n",
    "\n",
    "        # Track the PyTorch model architecture\n",
    "        writer.add_graph(model=model,\n",
    "                         # Pass in an example input\n",
    "                         input_to_model  = torch.randn(32, 3, 224, 224).to(device))\n",
    "        # Close the writer\n",
    "        writer.close()\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T09pmTaxfNl3"
   },
   "source": [
    "### 1.7 Choose loss function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uHknjZ6maOb"
   },
   "source": [
    "## 2. Training and Tracking models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7WlHYOKnP5P"
   },
   "outputs": [],
   "source": [
    "# 1. Create epochs list\n",
    "epochs = [15,25]\n",
    "\n",
    "\n",
    "# Create dictionary of our DataLoaders\n",
    "dataloaders = {\n",
    "    \"resnet18_dataloader\": resnet18_train_dataloader\n",
    "}\n",
    "\n",
    "model_dataloader_map = {\n",
    "    \"resnet18\": \"resnet18_dataloader\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527,
     "referenced_widgets": [
      "09f734d97a044ce1ad23965274450d77",
      "0565bfa36257493782c80b823bd704d3",
      "d95a378083ed489b998c31047c767cb6",
      "30d48f637c58449292da74605b010919",
      "622d08a3c5964ec28fd6eb93cb734e00",
      "251f6778afd64e92abeea5a55346381c",
      "a65ead747a5449bc9c753a5f1cb7e1b6",
      "8dd0807a2b6a447492a8e93803727087",
      "c2b3a262967745c6822d2ab8d108ad39",
      "75eee57503444dae8a1ba716d77814c1",
      "bf51fe8ffd3d4e7f855de9573c84112f"
     ]
    },
    "id": "HQSG4R-cuVek",
    "outputId": "9da62d34-ddd7-4d66-e2c5-a569ac4905d5"
   },
   "outputs": [],
   "source": [
    "from going_modular.going_modular.utils import save_model\n",
    "\n",
    "# 1. Set the random seeds\n",
    "set_seeds(seed=42)\n",
    "\n",
    "# 2. Keep track of experiment numbers\n",
    "experiment_number = 0\n",
    "\n",
    "# 3. Loop through each model and its corresponding DataLoader name\n",
    "for model_name, dataloader_name in model_dataloader_map.items():\n",
    "    train_dataloader = dataloaders[dataloader_name]  # Access the DataLoader from its name\n",
    "\n",
    "    # 4. Loop through each number of epochs\n",
    "    for epoch in epochs:\n",
    "        experiment_number += 1\n",
    "        print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "        print(f\"[INFO] Model: {model_name}\")\n",
    "        print(f\"[INFO] DataLoader: {dataloader_name}\")\n",
    "        print(f\"[INFO] Number of epochs: {epoch}\")\n",
    "\n",
    "        # 5. Select and create the model (make sure this is model *constructor*, not pretrained object)\n",
    "        if model_name == \"resnet18\":\n",
    "            selected_model = create_resnet18()\n",
    "        elif model_name == \"resnet34\":\n",
    "            selected_model = create_resnet34()\n",
    "        elif model_name == \"effnetb0\":\n",
    "            selected_model = create_effnetb0()\n",
    "        elif model_name == \"effnetb2\":\n",
    "            selected_model = create_effnetb2()\n",
    "        elif model_name == \"mobilenetv2\":\n",
    "            selected_model = create_mobilenetv2()\n",
    "        else:\n",
    "            raise ValueError(f\"[ERROR] Model '{model_name}' is not supported.\")\n",
    "\n",
    "        # 6. Create a new loss and optimizer for every model\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(selected_model.parameters(), lr=1e-3)\n",
    "\n",
    "        # 7. Train target model with target dataloaders and track experiments\n",
    "        train(model=selected_model,\n",
    "              train_dataloader=train_dataloader,\n",
    "              test_dataloader=mobile_test_dataloader,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=loss_fn,\n",
    "              epochs=epoch,\n",
    "              device=device,\n",
    "              writer=create_writer(\n",
    "                  experiment_name=dataloader_name,\n",
    "                  model_name=model_name,\n",
    "                  extra=f\"{epoch}_epochs\"\n",
    "              ))\n",
    "\n",
    "        # 8. Save the model to file\n",
    "        save_filepath = f\"Resistor_Predictor_{model_name}_{dataloader_name}_{epoch}_epochs.pth\"\n",
    "        save_model(model=selected_model,\n",
    "                   target_dir=\"models\",\n",
    "                   model_name=save_filepath)\n",
    "        print(\"-\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE7KsF8emTKB"
   },
   "source": [
    "It seems that the best accuracy my models can produce is an accuracy of up to 70%, The classifier layer may have been trained as much as possible.\n",
    "\n",
    " Let's try a phased approach for training the model. We will unfreeze the base model layers and train them after training the classifier layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEcDkbUKmsoa"
   },
   "source": [
    "## Two-Phase Training Loop\n",
    "\n",
    "Phase 1:\n",
    " * Train **only the classifier head** (with frozen base)\n",
    "\n",
    "Phase 2:\n",
    "  * Unfreeze the base, then fine-tune the **entire model at a lower learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVYN-9b6noT5",
    "outputId": "ad32313b-bc95-479d-e41d-681fd4f37bb9"
   },
   "outputs": [],
   "source": [
    "# Reinstantiate our models\n",
    "resnet18 = create_resnet18()\n",
    "resnet34 = create_resnet34()\n",
    "effnetb0 = create_effnetb0()\n",
    "effnetb2 = create_effnetb2()\n",
    "mobilenetv2 = create_mobilenetv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxwWzXec_QXf"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str,\n",
    "               test_loss: float = None,\n",
    "               test_accuracy: float = None):\n",
    "    \"\"\"\n",
    "    Saves a PyTorch model and optional metrics to a target directory.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch model.\n",
    "        target_dir: Directory to save the model file in.\n",
    "        model_name: Filename for the model (should end with .pth or .pt).\n",
    "        test_loss: Optional test loss to save with the model.\n",
    "        test_accuracy: Optional test accuracy to save with the model.\n",
    "    \"\"\"\n",
    "    # Create target directory if it doesn't exist\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \\\n",
    "        \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    # Prepare checkpoint dictionary\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "    }\n",
    "\n",
    "    if test_loss is not None:\n",
    "        checkpoint[\"test_loss\"] = test_loss\n",
    "    if test_accuracy is not None:\n",
    "        checkpoint[\"test_accuracy\"] = test_accuracy\n",
    "\n",
    "    # Save checkpoint\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(checkpoint, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510,
     "referenced_widgets": [
      "decafa712df843b7b2350032bba9a696",
      "63c1a7769c214f3baa484575811718e7",
      "894b23d9726642faa48b89ecb013b5dd",
      "eb7ff59008b54c7f8d871caa07196188",
      "0ab8b8ecf3fa46c785c649cfc27a3c67",
      "ad50163bbe4441ee9cd68f4641c19a36",
      "28202c8a2bec49e2a47dd39054e7f758",
      "f8fd26b6d02448a8ba2934eed574c78e",
      "75f6dc9f829d469f92570f990b585849",
      "598128e4bd5741069b367bac5a989691",
      "1bf9b2bd2f4c445cb899c405f1cdbfba"
     ]
    },
    "id": "tVHiX7ognfvg",
    "outputId": "a8dbbd2a-eb0c-4cfa-f690-5d158695a2e1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Set the random seeds\n",
    "set_seeds(seed=42)\n",
    "\n",
    "# 2. Keep track of experiment numbers\n",
    "experiment_number = 0\n",
    "\n",
    "# 3. Loop through each Dataloader\n",
    "for model_name, dataloader_name in model_dataloader_map.items():\n",
    "    train_dataloader = dataloaders[dataloader_name]  # Access the DataLoader from its name\n",
    "    # 4. Loop through each number of epochs\n",
    "    for epoch in epochs:\n",
    "        experiment_number += 1\n",
    "        print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "        print(f\"[INFO] Model: {model_name}\")\n",
    "        print(f\"[INFO] DataLoader: {dataloader_name}\")\n",
    "        print(f\"[INFO] Number of epochs per phase: {epoch}\")\n",
    "\n",
    "        # 5. Select and create the model (frozen base)\n",
    "        if model_name == \"resnet18\":\n",
    "            selected_model = create_resnet18()\n",
    "        elif model_name == \"resnet34\":\n",
    "            selected_model = create_resnet34()\n",
    "        elif model_name == \"effnetb0\":\n",
    "            selected_model = create_effnetb0()\n",
    "        elif model_name == \"effnetb2\":\n",
    "            selected_model = create_effnetb2()\n",
    "        elif model_name == \"mobilenetv2\":\n",
    "            selected_model = create_mobilenetv2()\n",
    "        else:\n",
    "            raise ValueError(f\"[ERROR] Model '{model_name}' is not supported.\")\n",
    "\n",
    "        # ----- Phase 1: Train classifier head only -----\n",
    "        print(\"[PHASE 1] Training classifier head (frozen base)...\")\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(selected_model.parameters(), lr=1e-3)\n",
    "\n",
    "        train(model=selected_model,\n",
    "              train_dataloader=train_dataloader,\n",
    "              test_dataloader=mobile_test_dataloader,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=loss_fn,\n",
    "              epochs=epoch,\n",
    "              device=device,\n",
    "              writer=create_writer(\n",
    "                  experiment_name=f\"{dataloader_name}_phase1\",\n",
    "                  model_name=model_name,\n",
    "                  extra=f\"{epoch}_epochs\"\n",
    "              ))\n",
    "\n",
    "         # ----- Phase 2: Fine-tune full model -----\n",
    "        print(\"[PHASE 2] Fine-tuning entire model (unfreezing base)...\")\n",
    "        for param in selected_model.parameters():\n",
    "            param.requires_grad = True  # unfreeze all layers\n",
    "\n",
    "        # Use a smaller learning rate for fine-tuning\n",
    "        optimizer_finetune = torch.optim.Adam(selected_model.parameters(), lr=1e-5)\n",
    "\n",
    "        train(model=selected_model,\n",
    "              train_dataloader=train_dataloader,\n",
    "              test_dataloader=mobile_test_dataloader,\n",
    "              optimizer=optimizer_finetune,\n",
    "              loss_fn=loss_fn,\n",
    "              epochs=epoch,\n",
    "              device=device,\n",
    "              writer=create_writer(\n",
    "                  experiment_name=f\"{dataloader_name}_phase2\",\n",
    "                  model_name=model_name,\n",
    "                  extra=f\"{epoch}_epochs_finetune\"\n",
    "              ))\n",
    "\n",
    "        # 6. Save the model to file\n",
    "        import os\n",
    "        os.makedirs(\"/content/drive/MyDrive/resistor_predictor_models\", exist_ok=True)\n",
    "\n",
    "\n",
    "        save_filepath = f\"Resistor_Predictor_{model_name}_{dataloader_name}_{epoch*2}_epochs.pth\"\n",
    "        save_model(model=selected_model,\n",
    "                   target_dir=\"/content/drive/MyDrive/resistor_predictor_models\",\n",
    "                   model_name=save_filepath)\n",
    "        print(\"-\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_igRJvt8oh5k"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir drive/MyDrive/resistor_predictor_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpXPdffGAOky"
   },
   "source": [
    "## 3. Load Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXhL_cBgymU6"
   },
   "outputs": [],
   "source": [
    "def inspect_and_load(model, filepath):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "        print(f\"[INFO] '{filepath}' is a checkpoint with 'model_state_dict'\")\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    elif isinstance(checkpoint, dict):\n",
    "        print(f\"[INFO] '{filepath}' is a plain state_dict (dictionary of weights)\")\n",
    "        model.load_state_dict(checkpoint)\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unknown checkpoint format in {filepath}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "eEMh7bWl_pnB",
    "outputId": "82978159-db0d-4256-cd29-3cd82305f617"
   },
   "outputs": [],
   "source": [
    "resnet18_loaded = inspect_and_load(create_resnet18(), \"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_resnet18_resnet18_dataloader_50_epochs.pth\")\n",
    "resnet34_loaded = inspect_and_load(create_resnet34(), \"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_resnet34_resnet34_dataloaders_50_epochs.pth\")\n",
    "mobilenetv2_loaded = inspect_and_load(create_mobilenetv2(), \"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_mobilenetv2_mobilenetv2_dataloaders_50_epochs.pth\")\n",
    "effnetb0_loaded = inspect_and_load(create_effnetb0(), \"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_effnetb0_effnetb0_dataloaders_50_epochs.pth\")\n",
    "effnetb2_loaded = inspect_and_load(create_effnetb2(), \"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_effnetb2_effnetb2_dataloaders_50_epochs.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzPfeIalGqcV"
   },
   "source": [
    "## 4. Make Predictions with our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvCunhKtJ7vg"
   },
   "outputs": [],
   "source": [
    "loaded_models = {\"resnet18\":resnet18_loaded,\n",
    "                 \"resnet34\": resnet34_loaded,\n",
    "                 \"mobilenetv2\":mobilenetv2_loaded,\n",
    "                 \"effnetb0\":effnetb0_loaded,\n",
    "                 \"effnetb2\":effnetb2_loaded}\n",
    "\n",
    "loaded_model_transforms = {\"resnet18\": resnet18_transforms,\n",
    "\"resnet34\":resnet34_transforms,\n",
    "\"mobilenetv2\":effnetb0_transforms,\n",
    "\"effnetb0\":effnetb2_transforms,\n",
    "\"effnetb2\":mobilenetv2_transforms}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43IW4eBHFoS0"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from typing  import List, Dict\n",
    "\n",
    "# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\n",
    "def pred_and_store(paths: List[pathlib.Path],\n",
    "                   model: torch.nn.Module,\n",
    "                   transform: torchvision.transforms,\n",
    "                   class_names: List[str],\n",
    "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
    "\n",
    "    # 2. Create an empty list to store prediction dictionaries\n",
    "    pred_list = []\n",
    "\n",
    "    # 3. Loop through target paths\n",
    "    for path in tqdm(paths):\n",
    "\n",
    "      # 4. Create empty dictionary to store prediction information for each sample\n",
    "      pred_dict = {}\n",
    "\n",
    "      # 5. Get the sample path and ground truth class name\n",
    "      pred_dict[\"image_path\"] = path\n",
    "      class_name = path.parent.stem\n",
    "      pred_dict[\"class_name\"] = class_name\n",
    "\n",
    "\n",
    "      # 6. Start the prediction timer\n",
    "      start_time = timer()\n",
    "\n",
    "      # 7. Open image path\n",
    "      img = Image.open(path)\n",
    "\n",
    "      # 8. Transform the image, add batch dimension and put image on target device\n",
    "      transformed_image = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "      # 9. Prepare model for inference by sending it to target device and turning one eval() mode\n",
    "      model.to(device)\n",
    "      model.eval()\n",
    "\n",
    "      # 10. Get prediction probality, prediction label and prediction class\n",
    "      with torch.inference_mode():\n",
    "        pred_logit = model(transformed_image) # perform inference on target sample\n",
    "        pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probailities\n",
    "        pred_label = torch.argmax(pred_prob, dim=1)  # turn prediction probailities into prediction label\n",
    "        pred_class = class_names[pred_label.cpu()]   # hardcode prediction class to be on CPU\n",
    "\n",
    "\n",
    "        #11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on)\n",
    "        with torch.inference_mode():\n",
    "          pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
    "          pred_dict[\"pred_class\"] = pred_class\n",
    "\n",
    "          # 12. End the timer and calculate the time per prediction\n",
    "          end_time = timer()\n",
    "          pred_dict[\"time_for_pred\"] = end_time-start_time\n",
    "\n",
    "        # 13. Does the pred match the true label?\n",
    "        pred_dict[\"correct\"] = class_name == pred_class\n",
    "\n",
    "        # 14. Add the dictionary to the list of preds\n",
    "        pred_list.append(pred_dict)\n",
    "\n",
    "  # 15. Return list of prediction dictionares\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f759CGNGWud"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\") )\n",
    "\n",
    "all_cpu_results = {}\n",
    "\n",
    "for loaded_model_name, loaded_model in tqdm(loaded_models.items()):\n",
    "  cpu_test_results = pred_and_store(paths=test_data_paths,\n",
    "                    model=loaded_model,\n",
    "                    transform=loaded_model_transforms[loaded_model_name],\n",
    "                    class_names=mobile_class_names,\n",
    "                    device=\"cpu\") # Make predictions on the cpu to replicate a mobile device\n",
    "\n",
    "  # Add the reults into final dictionary\n",
    "  all_cpu_results[loaded_model_name] = cpu_test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Z00ved0Mygi"
   },
   "outputs": [],
   "source": [
    "# Turn the all_cpu_results into a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "all_cpu_results_df = pd.DataFrame(all_cpu_results)\n",
    "all_cpu_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DteG0GENVQ3"
   },
   "outputs": [],
   "source": [
    "# Flatten results into a list of dictionaries, adding model name to each entry\n",
    "flat_results = []\n",
    "\n",
    "for model_name, preds in all_cpu_results_df.items():\n",
    "    for pred in preds:\n",
    "        pred[\"model_name\"] = model_name\n",
    "        flat_results.append(pred)\n",
    "\n",
    "# Convert to DataFrame\n",
    "flat_df = pd.DataFrame(flat_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1ciOAbNNp9z"
   },
   "outputs": [],
   "source": [
    "# Determine the predictions accuracy results and average prediction time\n",
    "model_prediction_stats =flat_df.groupby(\"model_name\")[\"correct\"].mean(),flat_df.groupby(\"model_name\")[\"time_for_pred\"].mean()\n",
    "model_prediction_stats = pd.DataFrame(model_prediction_stats).transpose()\n",
    "model_prediction_stats[\"correct\"] = round(model_prediction_stats[\"correct\"]*100,2)\n",
    "model_prediction_stats[\"time_for_pred\"] = round(model_prediction_stats[\"time_for_pred\"],4)\n",
    "model_prediction_stats.columns = [\"Accuracy(%)\", \"Average Prediction Time(s)\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoJ58RAiQqs2"
   },
   "outputs": [],
   "source": [
    "# Find the size of each of the models\n",
    "\n",
    "\n",
    "\n",
    "resnet18_path=\"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_resnet18_resnet18_dataloaders_50_epochs.pth\"\n",
    "resnet34_path=\"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_resnet34_resnet34_dataloaders_50_epochs.pth\"\n",
    "mobilenetv2_path = \"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_mobilenetv2_mobilenetv2_dataloaders_50_epochs.pth\"\n",
    "effnetb0_path =\"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_effnetb0_effnetb0_dataloaders_50_epochs.pth\"\n",
    "effnetb2_path = \"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_effnetb2_effnetb2_dataloaders_50_epochs.pth\"\n",
    "\n",
    "# Model path dictionary\n",
    "loaded_model_paths = {\"resnet18\": resnet18_path,\n",
    "\"resnet34\":resnet34_path,\n",
    "\"mobilenetv2\":effnetb0_path,\n",
    "\"effnetb0\":effnetb2_path,\n",
    "\"effnetb2\":mobilenetv2_path}\n",
    "\n",
    "# Model size dictironary\n",
    "loaded_model_size = {}\n",
    "\n",
    "for model_name, model_path in loaded_model_paths.items():\n",
    "  model_size = os.path.getsize(model_path)//(1024*1024)\n",
    "  loaded_model_size[model_name] = model_size\n",
    "\n",
    "model_prediction_stats = model_prediction_stats.join(pd.DataFrame(loaded_model_size, index=[\"Model Size(MB)\"]).transpose())\n",
    "model_prediction_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pn7rb4KLSjEu"
   },
   "outputs": [],
   "source": [
    "model_prediction_stats.drop([\"resnet18\", \"resnet34\",\"mobilenetv2\", \"effnetb0\", \"effnetb2\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7yMONLAUJ5z"
   },
   "outputs": [],
   "source": [
    "model_prediction_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Hfs5fJCi88P"
   },
   "source": [
    "Model Selection for Resistor Value Predictor (Visual CNN)\n",
    "\n",
    "| Model Name   | Accuracy (%) | Prediction Time (s) | Model Size (MB) |\n",
    "|--------------|--------------|--------------------|-----------------|\n",
    "| effnetb0     | 83.90        | 0.0435              | 30              |\n",
    "| effnetb2     | 80.68        | 0.0478              | 8               |\n",
    "| mobilenetv2  | 81.69        | 0.0318              | 15              |\n",
    "| resnet18     | 92.03        | 0.0283              | 42              |\n",
    "| resnet34     | 94.58        | 0.0418              | 81              |\n",
    "\n",
    "### Key Considerations:\n",
    "1. **Accuracy** — Critical for correct resistor value predictions.\n",
    "2. **Prediction Time** — Affects real-time user experience.\n",
    "3. **Model Size** — Impacts deployment cost, loading times, and mobile-friendliness.\n",
    "\n",
    "### Analysis:\n",
    "- **ResNet34**:\n",
    "  - **Highest accuracy (94.58%)**\n",
    "  - **Largest size (81MB)**\n",
    "  - Moderate prediction time.\n",
    "- **ResNet18**:\n",
    "  - **High accuracy (92.03%)**\n",
    "  - **Fastest prediction time (0.0283s)**\n",
    "  - Medium size (42MB)\n",
    "- **MobileNetV2**:\n",
    "  - Lightweight (15MB)\n",
    "  - Fast inference\n",
    "  - Lower accuracy (81.69%)\n",
    "- **EffNetB0 / EffNetB2**:\n",
    "  - Moderate size models\n",
    "  - Accuracy below 84%\n",
    "\n",
    "### Recommendation:\n",
    "- **Best Choice → ResNet18**\n",
    "  - High accuracy (92.03%)\n",
    "  - Fastest inference (0.0283s)\n",
    "  - Reasonable size (42MB)\n",
    "  \n",
    "- **If Model Size is Critical (e.g., mobile-first app)**:\n",
    "  - Consider **MobileNetV2** (15MB), but expect a ~10% drop in accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### Deployment Suggestion:\n",
    "- **For Web/Cloud Deployment**: ResNet18 is optimal.\n",
    "- **For Edge Devices (strict size constraints)**: MobileNetV2 is acceptable.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0cmCL3BjUQW"
   },
   "source": [
    "## 5. Deploy the ResNet18 Model into Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJIGaWEljewA",
    "outputId": "bafa8c48-3b56-4c6a-8221-53107c82a184"
   },
   "outputs": [],
   "source": [
    "# Import/Install Gradio\n",
    "\n",
    "try:\n",
    "  import gradio as gr\n",
    "except:\n",
    "  !pip install gradio\n",
    "  import gradio as gr\n",
    "\n",
    "print(f\"Gradio version: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEA7XSW9jzIO",
    "outputId": "4ffe1e4d-95d0-4df3-bb64-d10ad065b08f"
   },
   "outputs": [],
   "source": [
    "# Put resnet18 on cpu\n",
    "resnet18_loaded.to(\"cpu\")\n",
    "\n",
    "# Check the device\n",
    "next(iter(resnet18_loaded.parameters())).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LlFNK-uld-C"
   },
   "source": [
    "### 5.1 Creating a Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYI0_gqTkCus"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "  \"\"\"Transforms and performs a prediction on img and returns prediction and time taken\n",
    "  \"\"\"\n",
    "\n",
    "  # start the timer\n",
    "  start_time = timer()\n",
    "\n",
    "  # Tranform the target image and add a batch dimension\n",
    "  img = resnet18_transforms(img).unsqueeze(0)\n",
    "\n",
    "  # Put model into evaluation mode and turn on inference mode\n",
    "  resnet18_loaded.eval()\n",
    "  with torch.inference_mode():\n",
    "    # Pass the tranformed image through the model and turn the prediction logits into prediction probabilities\n",
    "    pred_probs = torch.softmax(resnet18_loaded(img), dim=1)\n",
    "\n",
    "  # Create a prediction label and prediction probaility dictionary for each prediction dictionary for each prediction class (this is the required format got Gradio's output parameter)\n",
    "  pred_labels_and_probs = {resnet18_class_names[i]: float(pred_probs[0][i]) for i in range(len(resnet18_class_names))}\n",
    "\n",
    "  # Calculate the prediction time\n",
    "  pred_time = round(timer() - start_time, 5)\n",
    "\n",
    "  # Return the prediction dictionary and prediction time\n",
    "  return pred_labels_and_probs, pred_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "levdQr_ZktTQ",
    "outputId": "8c220159-1afa-4166-9ec6-a0e0cc0bbf1e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Get a list of all test image filepaths\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\") )\n",
    "\n",
    "# randomly select a test image path\n",
    "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
    "\n",
    "# Open the target image\n",
    "image = Image.open(random_image_path)\n",
    "\n",
    "# Predict on the target image\n",
    "pred_dict, pred_time = predict(img=image)\n",
    "print(f\"Prediction label and probaility dictionary: \\n{pred_dict}\")\n",
    "print(f\"Prediction time: {pred_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDvktUYolZH5"
   },
   "source": [
    "### 5.2 Create a list of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4GT_BvRlsex",
    "outputId": "82f280ab-f491-46c1-9325-98f86fd86c35"
   },
   "outputs": [],
   "source": [
    "# Create a list of example inputs to our Gradio demo\n",
    "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=5)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMXElg0ymDX-"
   },
   "source": [
    "### 5.3 Building Our Gradio Interface\n",
    "\n",
    "Let's create a Gradio interface to replicate the workflow:\n",
    "\n",
    "```\n",
    "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken\n",
    "```\n",
    "\n",
    "We can do with the [`gradio.Interface()`](https://gradio.app/docs/#interface) class with the following parameters:\n",
    "* `fn` - a Python function to map `inputs` to `outputs`, in our case, we'll use our `predict()` function.\n",
    "* `inputs` - the input to our interface, such as an image using [`gradio.Image()`](https://gradio.app/docs/#image) or `\"image\"`.\n",
    "* `outputs` - the output of our interface once the `inputs` have gone through the `fn`, such as a label using [`gradio.Label()`](https://gradio.app/docs/#label) (for our model's predicted labels) or number using [`gradio.Number()`](https://gradio.app/docs/#number) (for our model's prediction time).\n",
    "    * **Note:** Gradio comes with many in-built `inputs` and `outputs` options known as [\"Components\"](https://gradio.app/docs/#components).\n",
    "* `examples` - a list of examples to showcase for the demo.\n",
    "* `title` - a string title of the demo.\n",
    "* `description` - a string description of the demo.\n",
    "* `article` - a reference note at the bottom of the demo.\n",
    "\n",
    "Once we've created our demo instance of `gr.Interface()`, we can bring it to life using [`gradio.Interface().launch()`](https://gradio.app/docs/#launch-header) or `demo.launch()` command.\n",
    "\n",
    "Easy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeuNJ7NpmJkk"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create title, desciption and article strings\n",
    "title = \"Resistor Predictor CNN 🎚️\"\n",
    "description = \"A ResNet18 feature extractor computer vision model to classify images of varying resistors to their corresponding resistor values\"\n",
    "article = \"Created by Murede A\"\n",
    "\n",
    "# Create the Gradio demo\n",
    "demo =gr.Interface(fn=predict, # mapping function from input to output\n",
    "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
    "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
    "                    examples=example_list,\n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "# Launch the demo!\n",
    "demo.launch(debug=False, # print errors locally?\n",
    "            share=True) # generate a publically shareable URL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLc_6ehWoPLR"
   },
   "source": [
    "## 6. Turning Resistor Predictor CNN into a deployable app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijVwSGmeongF"
   },
   "source": [
    "### 6.1 Creating a `demos` folder to store our Resistor Predictor CNN into a deployable app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0_wwLvto9eb"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create Resistor Predictor CNN demo path\n",
    "resistor_predictor_demo_path = Path(\"demos/resistor_predictor\")\n",
    "\n",
    "# Remove files that might already exist there and create new directory\n",
    "if resistor_predictor_demo_path.exists():\n",
    "  print(f\"{resistor_predictor_demo_path} directory exists. Removing...\")\n",
    "  shutil.rmtree(resistor_predictor_demo_path)\n",
    "\n",
    "# If the file doesn't exist, create it anyway\n",
    "resistor_predictor_demo_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check what's in the folder\n",
    "!ls demos/resistor_predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWUEOE_AqDTa"
   },
   "source": [
    "### 6.2  Create a folder of example images to use with our Resistor Predictor demo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-czaa83-qP3k",
    "outputId": "9de87c89-ada4-4b3c-a174-2cb6c1e1e89a"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Acreate an examples directory\n",
    "resistor_predictor_example_path = resistor_predictor_demo_path / \"examples\"\n",
    "resistor_predictor_example_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Collect three random test dataset image paths\n",
    "resistor_examples = [\n",
    "    Path('resistor_dataset/68K_1W/68K_1W_(12).jpg'),\n",
    "    Path('resistor_dataset/620R_1-4W/620R_1-4W_(3).jpg'),\n",
    "    Path('resistor_dataset/10R_2W/10R_2W_(5).jpg'),\n",
    "    Path('resistor_dataset/100R_1-4W/100R_1-4W_(2).jpg'),\n",
    "    Path('resistor_dataset/150R_1-8W/150R_1-8W_(7).jpg')\n",
    "]\n",
    "\n",
    "\n",
    "# 3. Copy the three random images to the examples directory\n",
    "for example in resistor_examples:\n",
    "  destination = resistor_predictor_example_path / example.name\n",
    "  print(f\"[INFO] Copying{example} to {destination}\")\n",
    "  shutil.copyfile(src=example, dst=destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NKoZZcbtazG",
    "outputId": "f014f67a-888e-4352-f384-0743a81ba5d8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get example filepaths in a list of lists\n",
    "example_list = [[\"examples/\" +example] for example in os.listdir(resistor_predictor_example_path)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epvbqE_vtpVW"
   },
   "source": [
    "### 6.3 Moving our trainned ResNet18 model to our  Resistor Predictor demo directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpC_wfbDxVBe",
    "outputId": "a8b0d85b-0ed4-4dac-c43d-b48e8c8d895f"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_resnet18_resnet18_dataloaders_50_epochs.pth\")\n",
    "print(file_path.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJ4j_5a3t4_O",
    "outputId": "2422ad37-d55b-4393-970e-5d04e38e1bb3"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the source model path\n",
    "resnet18_resistor_predictor_model_path = \"/content/drive/MyDrive/resistor_backup/resistor_predictor_models/Resistor_Predictor_resnet18_resnet18_dataloader_50_epochs.pth\"\n",
    "\n",
    "# Define temporary copy location (inside /content for example)\n",
    "temp_copy_path = Path(\"/content\") / Path(resnet18_resistor_predictor_model_path).name\n",
    "\n",
    "# Define destination path\n",
    "resistor_predictor_demo_path = Path(\"demos/resistor_predictor\")\n",
    "resistor_predictor_demo_path.mkdir(parents=True, exist_ok=True)  # make sure it exists\n",
    "resnet18_resistor_predictor_model_destination = resistor_predictor_demo_path / temp_copy_path.name\n",
    "\n",
    "# Step 1: Copy the file\n",
    "try:\n",
    "    print(f\"[INFO] Copying file to temporary location: {temp_copy_path}\")\n",
    "    shutil.copy2(src=resnet18_resistor_predictor_model_path, dst=temp_copy_path)\n",
    "    print(f\"[SUCCESS] File copied to: {temp_copy_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] Source file not found: {resnet18_resistor_predictor_model_path}\")\n",
    "except shutil.Error as e:\n",
    "    print(f\"[ERROR] shutil copy error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Unexpected error during copy: {e}\")\n",
    "\n",
    "# Step 2: Move the copied file to demo directory\n",
    "try:\n",
    "    print(f\"[INFO] Moving copied file to: {resnet18_resistor_predictor_model_destination}\")\n",
    "    shutil.move(src=temp_copy_path, dst=resnet18_resistor_predictor_model_destination)\n",
    "    print(f\"[SUCCESS] File moved to: {resnet18_resistor_predictor_model_destination}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] Copied file not found: {temp_copy_path}\")\n",
    "except shutil.Error as e:\n",
    "    print(f\"[ERROR] shutil move error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Unexpected error during move: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmXO1M19wNwN"
   },
   "source": [
    "### 6.4 Turning ResNet18 model into a Python script(model.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cns__Oldyfr5",
    "outputId": "bf2cdf84-d43b-4733-eeef-d5b7c28d19ff"
   },
   "outputs": [],
   "source": [
    "%%writefile demos/resistor_predictor/model.py\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "def create_resnet18():\n",
    "  # 1. Get the base model with pretrained weights and send it to target device\n",
    "  weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
    "  transforms = weights.transforms()\n",
    "  model = torchvision.models.resnet18(weights=weights).to(\"cpu\")\n",
    "\n",
    "  # 2. Freeze the base model layers\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  # 3. Set the seeds\n",
    "  torch.manual_seed(42)\n",
    "\n",
    "\n",
    "  # 4. Change the classifier head\n",
    "  model.fc = nn.Linear(in_features=512, out_features=37, bias=True)\n",
    "\n",
    "  return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOXeVhfEzHmd"
   },
   "source": [
    "### 6.5 Turning Resistor Predictor Gradio app into a Python Script (app.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUM8r_-xzUNO",
    "outputId": "d293b704-cd24-4d69-fca3-9299742a272f"
   },
   "outputs": [],
   "source": [
    "%%writefile demos/resistor_predictor/app.py\n",
    "\n",
    "### 1.  Imports and class names setup ###\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from model import create_resnet18\n",
    "import examples\n",
    "examples = \"examples\"\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Setup class names\n",
    "class_names = ['100R_1-4W',\n",
    " '10R_1W',\n",
    " '10R_2W',\n",
    " '10_1-4W',\n",
    " '11M_1-2W',\n",
    " '150R_1-4W',\n",
    " '150R_1-8W',\n",
    " '15R_1-4W',\n",
    " '180K_1-2W',\n",
    " '1K_1-4W',\n",
    " '1K_2W',\n",
    " '1M_1-4W',\n",
    " '20K_1-4W',\n",
    " '220K_1-4W',\n",
    " '220R_2W',\n",
    " '22R_1-4W',\n",
    " '24K_1-2W',\n",
    " '270K_1-4W',\n",
    " '27R_1W',\n",
    " '2K2_1-4W',\n",
    " '2R_1W',\n",
    " '330R_1-4W',\n",
    " '33K_2W',\n",
    " '3R9K_1-4W',\n",
    " '4700Mohm',\n",
    " '470R_1-4W',\n",
    " '470R_1W',\n",
    " '4K7_1-4W',\n",
    " '56K_1W',\n",
    " '5K1_1-4W',\n",
    " '5K61-4W',\n",
    " '620R_1-4W',\n",
    " '68K_1W',\n",
    " '6R8_1-4W',\n",
    " '7K5_1-4W',\n",
    " '820R_1-4W',\n",
    " '8K2_1-4W']\n",
    "\n",
    " ## 2. Model and transforms preparation ###\n",
    "\n",
    "# Create ResNet18 model\n",
    "resnet18, resnet18_transforms = create_resnet18()\n",
    "resnet18 = resnet18.to(\"cpu\")\n",
    "\n",
    "# Load saved weights\n",
    "model_path = Path(\"Resistor_Predictor_resnet18_resnet18_dataloader_50_epochs.pth\")\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "resnet18.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "### 3. Predict function ###\n",
    "\n",
    "# Create predict function\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "  \"\"\"Transforms and performs a prediction on img and returns prediction and time taken\n",
    "  \"\"\"\n",
    "\n",
    "  # start the timer\n",
    "  start_time = timer()\n",
    "\n",
    "  # Tranform the target image and add a batch dimension\n",
    "  img = resnet18_transforms(img).unsqueeze(0)\n",
    "\n",
    "  # Put model into evaluation mode and turn on inference mode\n",
    "  resnet18.eval()\n",
    "  with torch.inference_mode():\n",
    "    # Pass the tranformed image through the model and turn the prediction logits into prediction probabilities\n",
    "    pred_probs = torch.softmax(resnet18(img), dim=1)\n",
    "\n",
    "  # Create a prediction label and prediction probaility dictionary for each prediction dictionary for each prediction class (this is the required format got Gradio's output parameter)\n",
    "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "\n",
    "  # Calculate the prediction time\n",
    "  pred_time = round(timer() - start_time, 5)\n",
    "\n",
    "  # Return the prediction dictionary and prediction time\n",
    "  return pred_labels_and_probs, pred_time\n",
    "\n",
    "### 4. Gradio app ###\n",
    "\n",
    "# Create title, desciption and article strings\n",
    "title = \"Resistor Predictor CNN 🎚️\"\n",
    "description = \"A ResNet18 feature extractor computer vision model to classify images of varying resistors to their corresponding resistor values\"\n",
    "article = \"Created by Murede A\"\n",
    "\n",
    "# Gradio interface setup\n",
    "title = \"Resistor Predictor CNN 🎚️\"\n",
    "description = \"A ResNet18-based model that classifies resistor images.\"\n",
    "article = \"Created by Murede A\"\n",
    "examples = \"examples\"  # Or list of images like [[\"examples/1.jpg\"], ...]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
    "             gr.Number(label=\"Prediction time (s)\")],\n",
    "    examples=examples,\n",
    "    title=title,\n",
    "    description=description,\n",
    "    article=article\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=False, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFN4TV1qzlj1"
   },
   "source": [
    "### 6.6 Creating a requirements file for Resistor Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVduI_TKznpa",
    "outputId": "f5897c04-e41c-4d63-ac10-2fdb5ca5bd06"
   },
   "outputs": [],
   "source": [
    "%%writefile demos/resistor_predictor/requirements.txt\n",
    "torch==2.6.0\n",
    "torchvision==0.21.0\n",
    "gradio==5.31.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc_IH1iQ1bwf"
   },
   "source": [
    "### 7. Uploading our Resistor Predictor app into HuggingFace Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YksWQDov1zMh"
   },
   "source": [
    "These are all files that we've created!\n",
    "\n",
    "To begin uploading our files to Hugging Face, let's now download them from Google Colab (or wherever you're running this notebook).\n",
    "\n",
    "To do so, we'll first compress the files into a single zip folder via the command:\n",
    "\n",
    "```\n",
    "zip -r ../resistor_predictor.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
    "```\n",
    "\n",
    "Where:\n",
    "* `zip` stands for \"zip\" as in \"please zip together the files in the following directory\".\n",
    "* `-r` stands for \"recursive\" as in, \"go through all of the files in the target directory\".\n",
    "* `../resistor_predictor.zip` is the target directory we'd like our files to be zipped to.\n",
    "* `*` stands for \"all the files in the current directory\".\n",
    "* `-x` stands for \"exclude these files\".\n",
    "\n",
    "We can download our zip file from Google Colab using [`google.colab.files.download(\"demos/resistor_predictor.zip\")`](https://colab.research.google.com/notebooks/io.ipynb) (we'll put this inside a `try` and `except` block just in case we're not running the code inside Google Colab, and if so we'll print a message saying to manually download the files).\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "-fT_IpB-1oN8",
    "outputId": "0b347b58-e357-42bf-8b74-1999578b76fc"
   },
   "outputs": [],
   "source": [
    "# Change into and then zip the resistor_predictor folder but exclude certain files\n",
    "!cd demos/resistor_predictor && zip -r ../resistor_predictor.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
    "\n",
    "# Download the zipped Resistor  Predictor app (if running in Google Colab)\n",
    "try:\n",
    "  from google.colab import files\n",
    "  files.download(\"demos/resistor_predictor.zip\")\n",
    "except:\n",
    "  print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVITdmQl2IBt"
   },
   "source": [
    "### 7.2 Running Resistor Predictor demo locally\n",
    "\n",
    "If you download the `resistor_predictor.zip` file, you can test it locally by:\n",
    "1. Unzipping the file.\n",
    "2. Opening terminal or a command line prompt.\n",
    "3. Changing into the `resistor_predictor` directory (`cd resistor_predictor`).\n",
    "4. Creating an environment (`python -m venv env`).\n",
    "5. Activating the environment (`env\\Scripts\\activate`).\n",
    "5. Installing the requirements (`pip install -r requirements.txt`, the \"`-r`\" is for recursive).\n",
    "    * **Note:** This step may take 5-10 minutes depending on your internet connection. And if you're facing errors, you may need to upgrade `pip` first: `pip install --upgrade pip`.\n",
    "6. Run the app (`python app.py`).\n",
    "\n",
    "This should result in a Gradio demo just like the one we built above running locally on your machine at a URL such as `http://127.0.0.1:7860/`.\n",
    "\n",
    "> **Note:** If you run the app locally and you notice a `flagged/` directory appear, it contains samples that have been \"flagged\".\n",
    ">\n",
    "> For example, if someone tries the demo and the model produces an incorrect result, the sample can be \"flagged\" and reviewed for later.\n",
    ">\n",
    "> For more on flagging in Gradio, see the [flagging documentation](https://gradio.app/docs/#flagging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVd_v9As2eY6"
   },
   "source": [
    "### 9.3 Uploading to Hugging Face\n",
    "\n",
    "We've verified our Resistor Predictor  app works locally, however, the fun of creating a machine learning demo is to show it to other people and allow them to use it.\n",
    "\n",
    "To do so, we're going to upload our Resistor PRedictor demo to Hugging Face.\n",
    "\n",
    "> **Note:** The following series of steps uses a Git (a file tracking system) workflow. For more on how Git works, I'd recommend going through the [Git and GitHub for Beginners tutorial](https://youtu.be/RGOj5yH7evk) on freeCodeCamp.\n",
    "\n",
    "1. [Sign up](https://huggingface.co/join) for a Hugging Face account.\n",
    "2. Start a new Hugging Face Space by going to your profile and then [clicking \"New Space\"](https://huggingface.co/new-space).\n",
    "    * **Note:** A Space in Hugging Face is also known as a \"code repository\" (a place to store your code/files) or \"repo\" for short.\n",
    "3. Give the Space a name,\n",
    "4. Select a license (I used [MIT](https://opensource.org/licenses/MIT)).\n",
    "5. Select Gradio as the Space SDK (software development kit).\n",
    "   * **Note:** You can use other options such as Streamlit but since our app is built with Gradio, we'll stick with that.\n",
    "6. Choose whether your Space is it's public or private (I selected public since I'd like my Space to be available to others).\n",
    "7. Click \"Create Space\".\n",
    "8. Clone the repo locally by running something like: `git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]` in terminal or command prompt.\n",
    "    * **Note:** You can also add files via uploading them under the \"Files and versions\" tab.\n",
    "9. Copy/move the contents of the downloaded `resistor_predictor` folder to the cloned repo folder.\n",
    "10. To upload and track larger files (e.g. files over 10MB or in our case, our PyTorch model file) you'll need to [install Git LFS](https://git-lfs.github.com/) (which stands for \"git large file storage\").\n",
    "11. After you've installed Git LFS, you can activate it by running `git lfs install`.\n",
    "12. In the `resistor_predictor` directory, track the files over 10MB with Git LFS with `git lfs track \"*.file_extension\"`.\n",
    "    * Track EffNetB2 PyTorch model file with `git lfs track \"Resistor_Predictor_resnet18_resnet18_dataloaders_50_epochs.pth\"`.\n",
    "13. Track `.gitattributes` (automatically created when cloning from HuggingFace, this file will help ensure our larger files are tracked with Git LFS). You can see an example `.gitattributes` file on the [FoodVision Mini Hugging Face Space](https://huggingface.co/spaces/mrdbourke/foodvision_mini/blob/main/.gitattributes).\n",
    "    * `git add .gitattributes`\n",
    "14. Add the rest of the `resistor_predictor` app files and commit them with:\n",
    "    * `git add *`\n",
    "    * `git commit -m \"first commit\"`\n",
    "15. Push (upload) the files to Hugging Face:\n",
    "    * `git push`\n",
    "16. Wait 3-5 minutes for the build to happen (future builds are faster) and your app to become live!\n",
    "\n",
    "If everything worked, you should see a live running example of our FoodVision Mini Gradio demo like the one here: https://huggingface.co/spaces/mrdbourke/foodvision_mini\n",
    "\n",
    "And we can even embed our FoodVision Mini Gradio demo into our notebook as an [iframe](https://gradio.app/sharing_your_app/#embedding-with-iframes) with [`IPython.display.IFrame`](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.IFrame) and a link to our space in the format `https://hf.space/embed/[YOUR_USERNAME]/[YOUR_SPACE_NAME]/+`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hucpN4Ki3J3F"
   },
   "outputs": [],
   "source": [
    "# IPython is a library to help make Python interactive\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Embed Resistor Predictor Gradio demo\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0565bfa36257493782c80b823bd704d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_251f6778afd64e92abeea5a55346381c",
      "placeholder": "​",
      "style": "IPY_MODEL_a65ead747a5449bc9c753a5f1cb7e1b6",
      "value": " 13%"
     }
    },
    "09f734d97a044ce1ad23965274450d77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0565bfa36257493782c80b823bd704d3",
       "IPY_MODEL_d95a378083ed489b998c31047c767cb6",
       "IPY_MODEL_30d48f637c58449292da74605b010919"
      ],
      "layout": "IPY_MODEL_622d08a3c5964ec28fd6eb93cb734e00"
     }
    },
    "0ab8b8ecf3fa46c785c649cfc27a3c67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bf9b2bd2f4c445cb899c405f1cdbfba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "251f6778afd64e92abeea5a55346381c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28202c8a2bec49e2a47dd39054e7f758": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "30d48f637c58449292da74605b010919": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75eee57503444dae8a1ba716d77814c1",
      "placeholder": "​",
      "style": "IPY_MODEL_bf51fe8ffd3d4e7f855de9573c84112f",
      "value": " 2/15 [00:38&lt;03:04, 14.17s/it]"
     }
    },
    "598128e4bd5741069b367bac5a989691": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "622d08a3c5964ec28fd6eb93cb734e00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63c1a7769c214f3baa484575811718e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad50163bbe4441ee9cd68f4641c19a36",
      "placeholder": "​",
      "style": "IPY_MODEL_28202c8a2bec49e2a47dd39054e7f758",
      "value": "  0%"
     }
    },
    "75eee57503444dae8a1ba716d77814c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75f6dc9f829d469f92570f990b585849": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "894b23d9726642faa48b89ecb013b5dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8fd26b6d02448a8ba2934eed574c78e",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_75f6dc9f829d469f92570f990b585849",
      "value": 0
     }
    },
    "8dd0807a2b6a447492a8e93803727087": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a65ead747a5449bc9c753a5f1cb7e1b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad50163bbe4441ee9cd68f4641c19a36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf51fe8ffd3d4e7f855de9573c84112f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2b3a262967745c6822d2ab8d108ad39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d95a378083ed489b998c31047c767cb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8dd0807a2b6a447492a8e93803727087",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2b3a262967745c6822d2ab8d108ad39",
      "value": 2
     }
    },
    "decafa712df843b7b2350032bba9a696": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_63c1a7769c214f3baa484575811718e7",
       "IPY_MODEL_894b23d9726642faa48b89ecb013b5dd",
       "IPY_MODEL_eb7ff59008b54c7f8d871caa07196188"
      ],
      "layout": "IPY_MODEL_0ab8b8ecf3fa46c785c649cfc27a3c67"
     }
    },
    "eb7ff59008b54c7f8d871caa07196188": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_598128e4bd5741069b367bac5a989691",
      "placeholder": "​",
      "style": "IPY_MODEL_1bf9b2bd2f4c445cb899c405f1cdbfba",
      "value": " 0/15 [00:01&lt;?, ?it/s]"
     }
    },
    "f8fd26b6d02448a8ba2934eed574c78e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
